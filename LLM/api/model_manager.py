"""
Model Manager for EchoStory LLM API
Handles GGUF model loading, inference, and memory management for Mac M4
"""

import os
import gc
import asyncio
from typing import List, Dict, Any, Optional
from llama_cpp import Llama
import logging

# Try to import psutil, but make it optional
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

logger = logging.getLogger(__name__)

class ModelManager:
    def __init__(self):
        self.model = None
        self.model_name = None
        self.model_path = None
        self.loaded = False
        
        # Mac M4 ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏÑ§Ï†ï (10 ÎÖºÎ¶¨ ÏΩîÏñ¥) - 12B Î™®Îç∏Ïö© Ï°∞Ï†ï
        self.n_gpu_layers = 999  # Í∞ÄÎä•Ìïú Î™®Îì† Î†àÏù¥Ïñ¥Î•º GPU(Metal)Î°ú Ïò§ÌîÑÎ°úÎî©
        self.n_ctx = 4096  # Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Ï§ÑÏûÑ (12B Î™®Îç∏ÏùÄ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ ÌÅº)
        self.n_threads = 10  # ÎÖºÎ¶¨ ÏΩîÏñ¥ ÏàòÏôÄ ÎèôÏùº
        self.n_batch = 512  # Î∞∞Ïπò ÌÅ¨Í∏∞ Ï§ÑÏûÑ (Î©îÎ™®Î¶¨ Ï†àÏïΩ)
        self.n_ubatch = 128  # ÎßàÏù¥ÌÅ¨Î°úÎ∞∞Ïπò ÌÅ¨Í∏∞ Ï§ÑÏûÑ
        
        # Model configuration
        self.model_configs = {
            "I-Added-Glitter-Q4_K_M.gguf": {
                "name": "I-Added-Glitter-Q4",
                "type": "conversational",
                "max_length": 8192,
                "description": "I Added Glitter model with Q4_K_M quantization"
            },
            "MN-Violet-Lotus-12B.Q4_K_M.gguf": {
                "name": "MN-Violet-Lotus-12B-Q4",
                "type": "conversational",
                "max_length": 8192,
                "description": "MN Violet Lotus 12B model with Q4_K_M quantization"
            },
            "gemma-3n-E4B-it-Q4_K_M.gguf": {
                "name": "Gemma-3n-E4B-it-Q4",
                "type": "conversational",
                "max_length": 8192,
                "description": "Gemma 3n E4B instruction-tuned model with Q4_K_M quantization"
            },
            "BuRP_7B-Q5_K_M-imat.gguf": {
                "name": "BuRP-7B-Q5",
                "type": "conversational",
                "max_length": 4096,
                "description": "BuRP 7B model with Q5_K_M quantization"
            }
        }
        
        # Default model path
        self.default_model_file = "MN-Violet-Lotus-12B.Q4_K_M.gguf"
        self.models_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "models")
        
    async def load_model(self, model_file: Optional[str] = None):
        """Load the specified GGUF model"""
        if model_file is None:
            model_file = self.default_model_file
            
        # Construct full path to model
        self.model_path = os.path.join(self.models_dir, model_file)
        
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"Model file not found: {self.model_path}")
            
        logger.info(f"üîÑ Loading GGUF model: {model_file}")
        logger.info(f"üìÅ Model path: {self.model_path}")
        
        try:
            # Load GGUF model with Mac M4 optimized settings and stronger prompt adherence
            logger.info("üß† Loading GGUF model with llama-cpp-python...")
            self.model = Llama(
                model_path=self.model_path,
                n_gpu_layers=self.n_gpu_layers,  # Î™®Îì† Î†àÏù¥Ïñ¥Î•º Metal GPUÎ°ú Ïò§ÌîÑÎ°úÎî©
                n_ctx=self.n_ctx,  # Ïª®ÌÖçÏä§Ìä∏ ÏúàÎèÑÏö∞
                n_threads=self.n_threads,  # CPU Ïä§Î†àÎìú (ÎÖºÎ¶¨ ÏΩîÏñ¥ Ïàò)
                n_batch=self.n_batch,  # Î∞∞Ïπò ÌÅ¨Í∏∞ (Ï≤òÎ¶¨Îüâ Ìñ•ÏÉÅ)
                n_ubatch=self.n_ubatch,  # ÎßàÏù¥ÌÅ¨Î°úÎ∞∞Ïπò ÌÅ¨Í∏∞
                verbose=True,  # ÏÉÅÏÑ∏ Î°úÍπÖ ÌôúÏÑ±Ìôî
                use_mlock=True,  # Î™®Îç∏ÏùÑ Î©îÎ™®Î¶¨Ïóê Í≥†Ï†ï
                use_mmap=True,  # Î©îÎ™®Î¶¨ Îß§Ìïë ÏÇ¨Ïö©
                # Mac M4 Metal ÏµúÏ†ÅÌôî
                metal=True,  # Metal GPU ÏßÄÏõê ÌôúÏÑ±Ìôî
                f16_kv=True,  # 16ÎπÑÌä∏ ÌÇ§-Í∞í Ï∫êÏãú ÏÇ¨Ïö©
                # ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
                rope_scaling_type=1,  # RoPE Ïä§ÏºÄÏùºÎßÅ ÌôúÏÑ±Ìôî
                rope_freq_base=10000.0,  # RoPE Ï£ºÌååÏàò Î≤†Ïù¥Ïä§
                mul_mat_q=True,  # ÏñëÏûêÌôî ÌñâÎ†¨ Í≥±ÏÖà ÌôúÏÑ±Ìôî
                logits_all=False,  # ÎßàÏßÄÎßâ ÌÜ†ÌÅ∞Îßå Î°úÏßì Í≥ÑÏÇ∞ (Ìö®Ïú®ÏÑ± Ìñ•ÏÉÅ)
                embedding=False,  # ÏûÑÎ≤†Îî© Î™®Îìú ÎπÑÌôúÏÑ±Ìôî
                offload_kqv=True,  # ÌÇ§-Í∞í Ï∫êÏãúÎ•º GPUÎ°ú Ïò§ÌîÑÎ°úÎìú
                flash_attn=True,  # Flash Attention ÌôúÏÑ±Ìôî (ÏßÄÏõê Ïãú ÏÜçÎèÑ Ìñ•ÏÉÅ)
            )
            
            self.model_name = model_file
            self.loaded = True
            
            logger.info(f"‚úÖ GGUF model loaded successfully with Metal acceleration")
            logger.info(f"üìä Memory usage: {self.get_memory_usage()}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load GGUF model: {e}")
            raise
    
    def is_loaded(self) -> bool:
        """Check if model is loaded"""
        return self.loaded and self.model is not None
    
    def get_model_name(self) -> str:
        """Get the current model name"""
        if self.model_name and self.model_name in self.model_configs:
            return self.model_configs[self.model_name]["name"]
        return self.model_name or "unknown"
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """Get current memory usage"""
        memory_info = {}
        
        if HAS_PSUTIL:
            memory_info = {
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "memory_available_gb": psutil.virtual_memory().available / (1024**3),
                "memory_used_gb": psutil.virtual_memory().used / (1024**3)
            }
        else:
            memory_info = {
                "cpu_percent": "N/A",
                "memory_percent": "N/A", 
                "memory_available_gb": "N/A",
                "memory_used_gb": "N/A"
            }
        
        # Add model-specific memory info if available
        if self.model:
            try:
                # Get model memory usage from llama-cpp
                memory_info["model_loaded"] = True
                memory_info["model_path"] = self.model_path
            except:
                pass
        
        return memory_info
    
    async def generate_response(
        self, 
        messages: List[Dict[str, str]], 
        temperature: float = 0.8,
        max_tokens: int = 1024,
        top_p: float = 0.9
    ) -> str:
        """Generate response from messages using GGUF model"""
        if not self.is_loaded():
            raise RuntimeError("Model not loaded")
        
        try:
            # Format messages for the model
            formatted_prompt = self._format_messages(messages)
            logger.info(f"üìù Formatted prompt: {formatted_prompt[:200]}...")
            
            # Generate response using llama-cpp-python with MAXIMUM anti-repetition
            response = self.model(
                formatted_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                repeat_penalty=1.5,  # ÏµúÎåÄ Î∞òÎ≥µ Ìå®ÎÑêÌã∞
                top_k=50,  # ÏÉÅÏúÑ 50Í∞ú ÌÜ†ÌÅ∞ Í≥†Î†§
                stream=False,
                stop=["User:", "System:", "Human:", "Assistant:", "<end_of_turn>", "<start_of_turn>", "### USER MESSAGE", "### END"],
                echo=False,  # Don't echo the prompt
                # Maximum anti-repetition settings
                frequency_penalty=0.8,  # ÏµúÎåÄ ÎπàÎèÑ Ìå®ÎÑêÌã∞
                presence_penalty=0.8,   # ÏµúÎåÄ Ï°¥Ïû¨ Ìå®ÎÑêÌã∞
                typical_p=0.9,         # Typical sampling
                mirostat_mode=2,       # Mirostat sampling for better quality
                mirostat_tau=5.0,      # Target perplexity
                mirostat_eta=0.1,      # Learning rate
            )
            
            # Extract the generated text
            generated_text = response['choices'][0]['text']
            
            # Clean up response
            cleaned_response = self._clean_response(generated_text)
            
            logger.info(f"‚úÖ Generated response: {cleaned_response[:100]}...")
            return cleaned_response
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response: {e}")
            raise
    
    def _format_messages(self, messages: List[Dict[str, str]]) -> str:
        """Format messages for the model with stronger prompt adherence"""
        formatted_parts = []
        
        # Add EXTREMELY strong instruction header for roleplay
        formatted_parts.append("### ‚ö†Ô∏è ABSOLUTE CRITICAL ROLEPLAY INSTRUCTIONS - MANDATORY COMPLIANCE ‚ö†Ô∏è ###")
        formatted_parts.append("YOU ARE IN ROLEPLAY MODE. YOU MUST FOLLOW ALL INSTRUCTIONS EXACTLY.")
        formatted_parts.append("FAILURE TO COMPLY WITH FORMAT WILL RESULT IN REJECTION.\n")
        
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")
            
            if role == "system":
                # Make system messages EXTREMELY prominent
                formatted_parts.append("\n### üî¥ SYSTEM ROLEPLAY INSTRUCTIONS (ABSOLUTE PRIORITY) üî¥ ###")
                formatted_parts.append("YOU MUST FOLLOW THESE INSTRUCTIONS EXACTLY:")
                formatted_parts.append(f"{content}")
                formatted_parts.append("### END SYSTEM INSTRUCTIONS - THESE ARE YOUR ONLY RULES ###\n")
            elif role == "user":
                formatted_parts.append(f"\n### USER MESSAGE FROM {{{{user}}}} ###")
                formatted_parts.append(f"{content}")
                formatted_parts.append("### END USER MESSAGE ###\n")
            elif role == "assistant":
                formatted_parts.append(f"\n### PREVIOUS {{{{char}}}} RESPONSE ###")
                formatted_parts.append(f"{content}")
                formatted_parts.append("### END PREVIOUS RESPONSE ###\n")
        
        # Add EXTREMELY strong response instruction
        formatted_parts.append("\n### üé≠ NOW GENERATE YOUR ROLEPLAY RESPONSE AS {{char}} üé≠ ###")
        formatted_parts.append("REMEMBER: You are {{char}}, NOT an AI assistant!")
        formatted_parts.append("FORMAT: *actions* \"dialogue\" with thoughts and location at the end!")
        formatted_parts.append("MUST END WITH: *üí≠{{char}}'s Thoughts:* and *üìç{{char}}'s Location:*")
        formatted_parts.append("\n{{char}}'s response:")
        
        return "\n".join(formatted_parts)
    
    def _clean_response(self, response: str) -> str:
        """Clean and format the generated response with stronger filtering"""
        # Remove common artifacts
        response = response.strip()
        
        # Remove instruction markers that might leak into response
        instruction_markers = [
            "### ‚ö†Ô∏è ABSOLUTE CRITICAL ROLEPLAY INSTRUCTIONS - MANDATORY COMPLIANCE ‚ö†Ô∏è ###",
            "YOU ARE IN ROLEPLAY MODE",
            "FAILURE TO COMPLY",
            "### üî¥ SYSTEM ROLEPLAY INSTRUCTIONS (ABSOLUTE PRIORITY) üî¥ ###",
            "YOU MUST FOLLOW THESE INSTRUCTIONS EXACTLY:",
            "### END SYSTEM INSTRUCTIONS - THESE ARE YOUR ONLY RULES ###",
            "### USER MESSAGE FROM {{user}} ###",
            "### END USER MESSAGE ###",
            "### PREVIOUS {{char}} RESPONSE ###",
            "### END PREVIOUS RESPONSE ###",
            "### üé≠ NOW GENERATE YOUR ROLEPLAY RESPONSE AS {{char}} üé≠ ###",
            "REMEMBER: You are {{char}}, NOT an AI assistant!",
            "FORMAT: *actions*",
            "MUST END WITH:",
            "{{char}}'s response:",
            "### CRITICAL INSTRUCTIONS - MUST FOLLOW EXACTLY ###",
            "### SYSTEM INSTRUCTIONS (MANDATORY) ###",
            "### END SYSTEM INSTRUCTIONS ###",
            "### USER MESSAGE ###",
            "### ASSISTANT RESPONSE ###",
            "### END ASSISTANT RESPONSE ###",
            "### GENERATE RESPONSE (FOLLOW ALL INSTRUCTIONS ABOVE) ###"
        ]
        
        for marker in instruction_markers:
            response = response.replace(marker, "").strip()
        
        # Remove repeated patterns more aggressively
        lines = response.split('\n')
        cleaned_lines = []
        prev_line = ""
        
        for line in lines:
            line = line.strip()
            # Skip empty lines and exact duplicates
            if line and line != prev_line:
                # Also check for similar lines (to catch slight variations)
                is_similar = False
                for existing_line in cleaned_lines[-3:]:  # Check last 3 lines
                    if len(line) > 10 and len(existing_line) > 10:
                        # Simple similarity check
                        similarity = len(set(line.lower().split()) & set(existing_line.lower().split())) / max(len(line.split()), len(existing_line.split()))
                        if similarity > 0.8:
                            is_similar = True
                            break
                
                if not is_similar:
                    cleaned_lines.append(line)
                    prev_line = line
        
        response = '\n'.join(cleaned_lines)
        
        # Remove role prefixes if they appear in the response
        response = response.replace("Assistant:", "").strip()
        response = response.replace("User:", "").strip()
        response = response.replace("System:", "").strip()
        response = response.replace("Human:", "").strip()
        
        # Remove any remaining instruction-like text
        if response.startswith("###") or response.startswith("INSTRUCTIONS"):
            lines = response.split('\n')
            response = '\n'.join([line for line in lines if not line.strip().startswith("###") and not "INSTRUCTION" in line.upper()])
        
        return response.strip()
    
    def cleanup(self):
        """Clean up model and free memory"""
        logger.info("üßπ Cleaning up GGUF model...")
        
        if self.model is not None:
            # Close the model properly
            try:
                self.model.close()
            except:
                pass
            del self.model
            self.model = None
        
        # Force garbage collection
        gc.collect()
        
        self.loaded = False
        self.model_name = None
        self.model_path = None
        logger.info("‚úÖ GGUF model cleanup completed")
