"""
Backyard AI Bridge Server
Backyard AIì˜ ì›¹ì†Œì¼“ì´ë‚˜ ë‚´ë¶€ APIë¥¼ í™œìš©í•˜ëŠ” ë¸Œë¦¿ì§€ ì„œë²„
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import httpx
import asyncio
import json
import websocket
import threading

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str = "default"
    messages: List[Message]
    temperature: float = 0.7
    max_tokens: int = 1024
    top_p: float = 0.9
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    stream: bool = False

class ChatResponse(BaseModel):
    id: str = "chatcmpl-backyard"
    object: str = "chat.completion"
    created: int = 1234567890
    model: str = "backyard-model"
    choices: List[Dict[str, Any]]
    usage: Dict[str, int] = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

# Backyard AI í¬íŠ¸ë“¤ ì‹œë„ (13333 í¬íŠ¸ ì¶”ê°€)
BACKYARD_PORTS = [13333, 5001, 5000, 8080, 8081, 3000, 3001, 7860]

async def find_backyard_port():
    """Backyard AIê°€ ì‹¤í–‰ ì¤‘ì¸ í¬íŠ¸ ì°¾ê¸°"""
    for port in BACKYARD_PORTS:
        try:
            async with httpx.AsyncClient() as client:
                # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸ ì‹œë„ - v1/modelsë¥¼ ë¨¼ì € ì²´í¬
                endpoints = [
                    f"http://localhost:{port}/v1/models",
                    f"http://127.0.0.1:{port}/v1/models",
                    f"http://localhost:{port}/api/status",
                    f"http://localhost:{port}/status",
                    f"http://localhost:{port}/",
                ]
                
                for endpoint in endpoints:
                    try:
                        response = await client.get(endpoint, timeout=1.0)
                        if response.status_code < 500:
                            print(f"âœ… Backyard AI found at port {port}")
                            return port
                    except:
                        continue
        except:
            continue
    return None

@app.get("/")
async def root():
    port = await find_backyard_port()
    if port:
        return {"status": "Bridge server running", "backyard_port": port}
    return {"status": "Bridge server running", "backyard_port": "not found", "message": "Please ensure Backyard AI is running"}

@app.get("/v1/models")
async def list_models():
    """ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return {
        "object": "list",
        "data": [
            {
                "id": "backyard-model",
                "object": "model",
                "created": 1234567890,
                "owned_by": "backyard"
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """
    OpenAI í˜¸í™˜ ì±„íŒ… ì™„ì„± ì—”ë“œí¬ì¸íŠ¸
    Backyard AIì™€ í†µì‹  ì‹œë„
    """
    
    # 1. ë¨¼ì € Backyard AI í¬íŠ¸ ì°¾ê¸°
    port = await find_backyard_port()
    
    if port:
        # Backyard AIì™€ í†µì‹  ì‹œë„
        async with httpx.AsyncClient() as client:
            try:
                # Backyard AIì˜ ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸ë“¤ - v1/chat/completionsë¥¼ ë¨¼ì € ì‹œë„
                endpoints = [
                    f"http://localhost:{port}/v1/chat/completions",
                    f"http://127.0.0.1:{port}/v1/chat/completions",
                    f"http://localhost:{port}/api/v1/chat/completions",
                    f"http://localhost:{port}/api/chat",
                    f"http://localhost:{port}/chat",
                ]
                
                for endpoint in endpoints:
                    try:
                        response = await client.post(
                            endpoint,
                            json=request.dict(),
                            timeout=30.0
                        )
                        if response.status_code == 200:
                            return response.json()
                    except Exception as e:
                        print(f"Endpoint {endpoint} failed: {e}")
                        continue
            except Exception as e:
                print(f"Error communicating with Backyard AI: {e}")
    
    # Backyard AIì™€ í†µì‹  ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ
    # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ Backyard AIì™€ í†µì‹  ì‹œë„
    user_message = request.messages[-1].content if request.messages else ""
    
    # ê°„ë‹¨í•œ í´ë°± ì‘ë‹µ (ì–¸ì–´ ê°ì§€)
    is_korean = any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in user_message)
    
    if is_korean:
        fallback_response = f"[ë¸Œë¦¿ì§€ ì„œë²„] Backyard AI ì—°ê²° ëŒ€ê¸° ì¤‘... ì‚¬ìš©ì ë©”ì‹œì§€: {user_message[:50]}..."
    else:
        fallback_response = f"[Bridge Server] Waiting for Backyard AI connection... User message: {user_message[:50]}..."
    
    return ChatResponse(
        choices=[{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": fallback_response
            },
            "finish_reason": "stop"
        }]
    )

if __name__ == "__main__":
    import uvicorn
    print("ğŸŒ‰ Backyard AI Bridge Server starting on port 8000...")
    print("ğŸ“ Instructions:")
    print("1. Make sure Backyard AI is running")
    print("2. This bridge will try to find and connect to Backyard AI")
    print("3. Use http://localhost:8000/v1/chat/completions for API calls")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
